직렬화란?

    객체의 직렬화는 객체의 내용을 바이트 단위로 변환하여 파일 또는 네트워크를 통해서 스트림(송수신)이 가능하도록 하는 것을 의미한다.

YAML (yml 파일)

    YAML은 XML, C, 파이썬, 펄, RFC2822에서 정의된 e-mail 양식에서 개념을 얻어 만들어진 '사람이 쉽게 읽을 수 있는' 데이터 직렬화 양식이다. 
    오늘날 XML과 JSON이 데이터 직렬화에 주로 쓰이기 시작하면서, 많은 사람들이 YAML을 '가벼운 마크업 언어'로 사용하려 하고 있다.

ELK STACK이란?

    logstash, elasticsearch, kibana 이 3가지를 말하는 것으로 데이터 분석에 쓰이는 도구들이다.
    logstash가 여러 정보를 수집해서 변환 후 elasticsearch에 보내주고,
    elasticsearch는 필요에 맞게(kibana에 맞게) 데이터 정리 하면 그 데이터를 kibana가 시각화 한다.

Logstash

    데이터를 ElasticSearch에 수집해준다

Kibana

    ElasticSearch를 화면에 보기좋게 보여준다.
    데이터의 시각화를 담당한다.

ElasticSearch

    Big-O notation을 보면 ElasticSearch 는 해쉬테이블과 같다. O(1)

    해쉬?

        해시함수(hash function)란 데이터의 효율적 관리를 목적으로 임의의 길이의 데이터를 고정된 길이의 데이터로 매핑하는 함수이다.
        이 때 매핑 전 원래 데이터의 값을 키(key), 매핑 후 데이터의 값을 해시값(hash value), 매핑하는 과정 자체를 해싱(hashing)라고 합니다.

        해시함수는 해쉬값의 개수보다 대개 많은 키값을 해쉬값으로 변환(many-to-one 대응)하기 때문에 해시함수가 서로 다른 두 개의 키에 대해 동일한 해시값을 내는 해시충돌(collision)이 발생하게 됩니다. 

    ElasticSearch는 관계형데이터베이스(RDB)보다 탐색이 훨씬 더 빠르다.

    ElasticSearch는 REST API를 사용한다.

    ElasticSearch vs Relational DB -   CRUD

        Index           Database
        Type             Table
        Document          Row
        Field            Column
        Mapping          Schema

        GET              Select       Read
        PUT              Update       Update
        POST             Insert       Create
        DELETE           Delete       Delete

    시작하기

        sudo service elasticsearch start
        sudo service elasticsearch stop
        curl -XGET 'localhost:9200' # check if elasticsearch run

    

    * curl 을 이용하여 REST API를 확인해보자.

        요청 보내기
        
            REST API를 사용할 때는 cURL 옵션 중 몇 가지를 꼭 알아두는 것이 좋다. 

        prefix

            -i: 응답 헤더 출력 (옵션 없으면 응답 본문만 출력함)
            -v: 중간 처리 과정, 오류 메시지, 요청 메시지와 응답 메시지를 헤더와 본문을 포함해 전체 출력
            -X: 요청 메소드를 지정 (옵션 없으면 기본값은 GET)
            -H: 요청 헤더를 지정
            -d: 요청 본문을 지정 (옵션 없으면 요청 본문 없음) FORM 을 POST 하는 HTTP나 JSON 으로 데이타를 주고받는 REST 기반의 웹서비스 디버깅시 유용한 옵션이다
            -s:	--silent 정숙 모드. 진행 내역이나 메시지등을 출력하지 않는다. HTTP response code 만 가져오거나 할 경우 유리

        * uri 파라미터에 ?pretty 라고 쳐주면 결과값을 더 깔끔하게 출력할 수 있다!

        6.0 버전부터는 요청 헤더에 content-type를 명시한다.
    
            ex ) curl -XPOST http://locahost:9200/classes/class/1/ -H 'Content-Type: application/json' -d '{“title” : "Algorithm", "professor" : "John"}'

        VERIFY INDEX4

            curl -XGET http://localhost:9200/classes
                        엘라스틱 서치        /  Index name

            curl -XGET http://localhost:9200/classes?pretty ( 좀더 깔끔하게 출력된다. )

        CREATE INDEX

            curl -XPUT http://localhost:9200/classes

        DELETE INDEX

            curl -XDELETE http://localhost:9200/classes

        CREATE DOCUMENT

            curl -XPOST http://localhost:9200/classes/class/1/ -H 'Content-Type: application/json' -d '{"title" : "Algorithm", "professor" : "John"}'

        CREATE INDEX, TYPE, DOCUMENT FROM FILE (파일로부터 생성)

            curl -XPOST http://localhost:9200/classes/class/1 -d @oneclass.json

        ADD ONE MORE FIELD

            curl -XPOST http://localhost:9200/classes/class/1/_update -H 'Content-Type: application/json' -d '{"doc" : {"unit" : 1}}'

        UPDATE ONE FIELD

            curl -XPOST http://localhost:9200/classes/class/1_update -H 'Content-Type: application/json' -d '{"doc" : {"unit":2}}'

        UPDATE ONE FIELD WITH SCRIPT

            curl -XPOST http://localhost:9200/classes/class/1_update -H 'Content-Type: application/json' -d '{"script" : "ctx._source.unit += 5"}'

여러 개의 document를 한번에 ElasticSearch에 삽입하는 방법

    
    ex ) CLASSES.JSON (BULK FILE)

        curl -s -H “Content-Type: application/x-ndjson” -XPOST localhost:9200/_bulk -–data-binary “@classes.json”

        벌크옵션 주어서 파일에서 데이터 바이너리를 불러온다. (파일에서 직접 뽑아내는 것이기 때문에 --data-binary 옵션을 꼭 주어야 한다!)
        여러 개의 document를 한번에 삽입할 수 있다!

    BULK FILE은 두개의 라인으로 구성되어 있다.
    
        첫번째는 메타 인포메이션이다. (어떤 인덱스, 어떤 타입, 어떤 아이디 등등)
        두번째는 document.

엘라스틱서치 매핑(Mapping)

    elastic search 는 dynamic mapping 또는 explicit mapping 이 있다. 
    dynamic mapping은 json 파일을 입력하면 자동으로 mapping type이 정해지는것이고 
    explicit mapping 은 필드마다 매핑 타입을 따로 지정해주는 것이다. (공식 문서 참조)

    실제로 일할 때에는 매핑을 넣어야 한다! (안넣으면 위험함.)
    잘못 지정된 타입같은 경우에는 kibana같이 시각화 할 때 문제가 생긴다.
    숫자인데 문자열로 저장된다거나 하면 평균이나 어떠한 계산 값을 할 때 문제가 생긴다.

    명시적 매핑을 할 때 일일이 필드 이름 타이핑 하기 번거로우니
    일단 bulk index로 dynamic mapping을 한 후 위 방법으로 mapping을 확인하거나 
    kibana에서 mapping을 받아와서 수정하고 싶은 부분만 수정하도록 한다.

    아무튼 데이터를 관리할 때는 항상 매핑을 추가해야 한다. (또는 추후에 꼭 추가한다.)
    그래야 시각화 할 때, 분석할 때 도움을 받을 수 있다.

    CREATE MAPPING (명시적 매핑 explicit mapping)

        ex) curl -XPUT 'http://localhost:9200/classes/class/_mapping' -H 'Content-Type: application/json' -d @classesRating_mapping.json

    ADD DOCUMENTS (명시적 매핑 후 다량의 데이터 추가 - 벌크)

        curl -XPOST http://localhost:9200/bulk?pretty -H 'Content-Type: application/json' --data-binary @classes.json

    VERIFY DOCUMENTS

        curl -XGET http://localhost:9200/classes/class/1/?pretty

HOW TO STORE DATA FIELD TYPE

        {
            "class" : {
                "properties" : {
                    "title" : {
                        "type" : "text"
                    },
                    "professor" : {
                        "type" : "text"
                    },
                    "major" : {
                        "type" : "text"
                    },
                    "semester" : {
                        "type" : "text"
                    },
                    "student_count" : {
                        "type" : "integer"
                    },
                    "unit" : {
                        "type" : "integer"
                    },
                    "rating" : {
                        "type" : "integer"
                    },
                    "submit_date" : { // 타입을 date, 데이트 포맷을 써주면 시각화할 때 날짜별로 쪼개서 보여줄 수 있다. 
                        "type" : "date",
                        "format" : "yyyy-MM-dd"
                    },
                    "school_location" : { // Kibana에서 데이터를 시각화할 때 geo_point 쓰면 지도위에 직접 표시 가능
                        "type" : "geo_point"
                    }
                }
            }
        }

        * string 타입 때문에 에러나는 경우 unix 계열에서는 다음과 같은 명령어로 string 을 text 로 변환하자.
            6.1버전 기준으로 string 타입은 에러발생! text 타입을 사용하면 된다.

        sed “s/string/text/” classesRating_mapping.json | tee classesRating_mapping.json

    엘라스틱 데이터 조회 (Search)

        1. 벌크 파일을 우선 삽입한다.

            ADD DOCUMENTS 

                ex) curl -XPOST 'localhost:9200/_bulk' -H 'Content-Type: application/json' --data-binary @simple_basketball.json

        2. SEARCH

                ex) curl -XGET localhost:9200/basketball/record/_search?pretty

                    다큐먼트들을 확인할 수 있다.

        3. SEARCH - URI

            curl -XGET 'localhost:9200/basketball/record/_search?q=points:30&pretty'

        4. SEARCH - REQUEST BODY (다이렉트하게 직접적으로 찾는 방법, 도큐먼트를 출력한다.)

            curl -XGET 'localhost:9200/basketball/record/_search -H Content-Type: application/json -d '
            {
                "query":{
                    "term" : {"points":30}
                }
            }'

            리퀘스트 바디에는 여러가지 방법들이 있다. 이것 또한 방법 중 하나이다.

    AGGREGATIONS

        엘라스틱서치에서 어그리게이션은 DOCUMENT 안에서 조합을 통해서 값을 도출한다. (BigData ch03폴더 참고)

        어떠한 것을 조합해서 나타낸다.

        그중에서 METRIC AGGREGATIONS은 산수를 할 때 쓰인다. (ex - 평균, 최솟값, 최댓값 등등)


        * Structuring Aggregations

            "aggregations" : {
                "<aggregation_name>" : {
                    "<aggregation_type>" : {
                        <aggregation_body>
                    }
                    [,"meta" : {  [<meta_data_body>] } ]?
                    [,"aggregations" : { [<sub_aggregation>]+ } ]?
                }
                [,"<aggregation_name_2>" : { ... } ]*
            }

        * Metric Aggregation

            ADD BASKETBALL DOCUMENTS

                -curl -XPOST 'localhost:9200/_bulk' -H "Content-Type:application/json" --data-binary @ simple_basketball.json

            * json 파일 안에서 "aggs" 로 키 값을 설정해주면 어그리게이션을 나타낸다.

            AVERAGE

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @avg_points_aggs.json

            MAX

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @max_points_aggs.json

            MIN

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @min_points_aggs.json

            SUM

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @sum_points_aggs.json

            STATS (전부 출력됨)

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @stats_points_aggs.json

        * Bucket Aggregation (group by)

            각각 팀별로 어떠한 결과 값을 도출하고 싶다면 버켓 어그리게이션을 사용한다.

            * 버켓 어그리게이션 같은 경우에는 서브 어그리게이션을 포함할 수 있다. ex) 버켓 안에 메트릭 어그리게이션 가능. ch04의 stats_by_team.json 참고.

            CREATE BASKETBALL INDEX
            
                curl -XPUT localhost:9200/basketball

            MAPPING BASKETBALL TYPE

                basketball_mapping.json 확인

                fielddata가 true이면 어그리게이션 할 때 사용 가능.

                curl -XPUT 'localhost:9200/basketball/record/_mapping' -H 'Content-Type: application/json' -d @basketball_mapping.json

            SAMPLE DATA - BASKETBALL DOCUMENTS

                vi twoteam_basketball.json

            ADD BASKETBALL DOCUMENTS

                curl -XPOST 'localhost:9200/_bulk' -H "Content-Type:application/json" --data-binary @twoteam_basketball.json

            TERM AGGS (GROUP BY TEAM)

                vi terms_aggs.json 명령어로 확인

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @terms_aggs.json

                team에 관련된 어그리게이션을 통한 값이 도출된다.

            AGGS (STATS GROUP BY TEAM)

                vi stats_by_team.json

                * 버켓 어그리게이션 같은 경우에는 서브 어그리게이션을 포함할 수 있다.

                curl -XGET localhost:9200/_search?pretty -H "Content-Type:application/json" --data-binary @stats_by_team.json (결과 값 확인)

                단순히 결과값이 DOCUMENT뿐이였지만, 서브 어그리게이션인 메트릭 어그리게이션까지 나타내게 된다.

------------------------------------------------------------------------------------------------------------------------------------------------

Kibana

        INSTALL KIBANA

            1. Download DEB file from https://www.elastic.co/downloads/kibana

            2. sudo dpkg -i kibana-6.5.4-amd64.deb

        CONFIG KIBANA (키바나 설정 /ETC/KIBANA/KIBANA.YML에서 설정할 것)

            * 키바나와 엘라스틱 버젼은 동일해야 한다.

            sudo nano /ETC/KIBANA/KIBANA.YML (파일 열어서 수정)

                # elasticsearch.url: YOUR_ELASTICSEARCH_URL (http://localhost:9200)
                # server.host: the address to which the kibana server will bind (localhost)

                위 두개 사항을 상황에 맞게 수정해줄 것.

        START KIBANA (USR/SHARE/KIBANA/BIN/KIBANA)

            sudo /usr/share/kibana/bin/kibana

Kibana management

    키바나 관리 (인덱스부터 설정한 뒤 필드네임 설정한다.)

Kibana discover

    Time Range, 필터 기능을 이용하여 원하는 필드의 데이타를 확인할 수 있다.

Kibana Visualize - 막대그래프, 파이차트

    Buckets Aggregation 으로 Terms과 필드,

    Metrics Aggregation으로 원하는 어그리게이션과 필드를 설정하고 시각화하여 확인할 수 있다.

Kibana Visualize - 지도에 표시 (BigData ch02 폴더 활용) / coordinate map

    CREATE INDEX

        curl -XPUT 'http://localhost:9200/classes'

        컬 명령어를 통해 인덱스를 생성한다.

    CREATE MAPPING (ch02 폴더 참고)

        curl -XPUT 'http://localhost:9200/classes/class/_mapping' -H 'Content-Type:application/json' -d @classesRating_mapping.json

        단순한 text인지, 아니면 geo_point인지 모르기 때문에 맵핑을 함으로써 지도에 표시할 수 있는 데이터인지 아닌지 알려주어야 한다.

    VERIFY MAPPING

        curl -XGET http://localhost:9200/classes/?pretty

        맵핑되었는지 확인

    ADD DOCUMENTS

        curl -XPOST http://localhost:9200/_bulk?pretty -H 'Content-Type:application/json' --data-binary @classes.json

    VERIFY DOCUMENTS

        curl -XGET http://localhost:9200/classes/class/1/?pretty

        도큐먼트 확인

Kibana Dashboard

    Save기능으로 차트를 저장할 수 있다.
    대시보드에서 전체적인 차트들을 펼쳐 볼 수 있음.

------------------------------------------------------------------------------------------------------------------------------------------------

리눅스 명령어 참고

    bg - 백그라운드 실행 확인
    ctrl + z - 실행 중인 프로그램을 백그라운드로.
    fg%1 첫번째 해당 백그라운드 프로그램 재시작
    fg - 포어그라운드 시킴

------------------------------------------------------------------------------------------------------------------------------------------------

LOGSTASH 

    ELK-STACK에서 LOGSTASH는 INPUT을 담당한다.
    로그스태쉬에서 받은 인풋은 변환되어 엘라스틱서치에 들어가게 되고,
    키바나는 바로 그 데이터를 시각화하게 된다.

    상당히 많은 데이터를 흡수, 변환시켜 ElasticSearch에 보낼 수 있다. (MySQL, mongoDB 등..)

    ex) csv에 있는 값들은 모두 텍스트 값인데 수치로 나타내고 싶을 경우
        로그스테쉬에서 수치로 변환 시킬 수 있음.

    * 인스톨하기전에 역시 자바설치 필요

    INSTALL LOGSTASH

        sudo dpkg -i logstash-6.5.4.deb

    CONFIG LOGSTASH

        input으로 키보드에서 stdin을 받고,
        output으로 모니터로 스탠다드아웃, stdout을 보낸다.

    RUN LOGSTASH

        /usr/share/logstash/bin$ sudo ./logstash -f 디렉토리경로/logstash-simple.conf

REAL POPULATION ANALYSIS WITH ELK STACK (ch06 logstash.conf 참고)

    polulation by country

    리눅스 명령어 팁
    
        ps -ef 프로세스 다 보기

        ps -ef | grep kibana


    LOGSTASH.CONF
    
        start_position 

            처음부터 파일을 읽게 한다. beginning

            디폴트는 end로 되어있음.

        sincedb_path

            처음에는 데이터가 잘 들어가지만, 한번 들어간 데이터는 로그스태쉬가 넣지 않는다.
            sincedb_path 를 /dev/null 이런 식으로 설정해준다면 로그스태쉬로 데이터를 넣을 때마다
            똑같은 데이터가 잘 삽입되는 것을 볼 수 있을 것이다. (다음에 넣을 데이터가 중복된 데이터여도 허용된다.)
    
        filter

            csv separator 를 콤마로 설정.
            column 설정.
            숫자로 표시하기 위해서 float 설정.

        output

            elasticsearch로 보낸다.
            호스트 설정, 인덱스 설정 해주자.

            output {  
                elasticsearch {
                    hosts => "localhost"
                    index => "population"
                }
                stdout {}
            }

    RUN LOGSTASH OUTPUT TO ELASTICSEARCH

        /usr/share/logstash/bin 디렉토리에 위치한 후,
        
        아래의 명령어 실행

            sudo ./logstash -f ~/.conf파일위치

        * 로그스태쉬의 PATH 는 절대 경로만 지원한다.
          CONFIG 파일의 절대 경로가 맞는지 꼭 확인할 것.

    주식, 인구등 csv 데이터들을 로그스태쉬로 엘라스틱서치에 맞게 변형하고,
    그것을 kibana로 시각화하여 확인할 수 있다!

---------------------------------------------------------------

클라우드 로그 분석 시스템 아키텍처 ( ELK STACK FOR CLOUD LOG ANALYSIS )

    * 엘라스틱 서치는 로그를 저장하는 데이터베이스로 활용된다.

    * 로그스테쉬는 클라우드 내에서 발생된 로그를 전달받아서 엘라스틱서치에 저장하는 역할을 한다.

    * 키바나는 엘라스틱 서치에 저장된 로그를 브라우저에 출력한다.

    ELK-STACK을 로그분석 시스템으로 사용할 경우, 브라우저를 통해서 클라우드 상의 모든 로그를
    실시간으로 활용할 수 있다.

    이러한 이유로 엘크스텍은 최근 중앙 로그 분석 시스템으로 각광받고 있다.

    로그스태쉬는 어떻게 클라이언트 상에 정보를 입력받을 수 있을까?
    정답은 filebeat 이다.

    각각의 서버에 설치되어서 로그에 변화가 있을 때 변화된 파일을 로그스태쉬에 전달해준다.
    실무에서 엘크스텍 사용시 오래된 데이터를 삭제하지 않아서 디스크 공간 문제가 생길 수 있다.
    이런 문제는 큐레이터를 사용하면 쉽게 해결된다.
    손쉽게 데이터 보존기간 및 데이터 최대 사용량을 설정해서 엘크스텍에 디스크 공간 문제가 없도록 해준다.

    큐레이터가 최대 보존기간이 1달이라고 가정했을 때,
    실제 한달 이상된 로그를 봐야하는 경우가 생기는 경우에는
    Amazon S3에 저장하여 필요시 한달 이상된 로그를 엘크스텍에 복구하는
    엘크스텍을 구현한다면 보다 완벽한 시스템이라고 할 수 있다.

    Curator - Delete old data (size base, time base)

    filebeat - Tomcat logs, db logs, etc..

    Amazon S3 - Backup / restore using 

엘라스틱 서치는 데이터 분석해주는 도구들 중 데이터를 정리하는 역할
    
---------------------------------------------------------------

FileBeat로 분산 서버 로그 ELK스텍에 전달하기 - FILEBEAT WILL SEND LOGS TO LOGSTASH

큐레이터로 ELK스텍 디스크 공간 자동으로 관리하기

S3를 활용한 ELK 스텍 로그 백업 및 복원

.

백업 스크립트 / 크론 탭 추후 복습할 것.